# AWQ-4bits-on-small-LLMs
This repo tries to implement Activation-aware Weight Quantization for LLM Compression and Acceleration from scratch and apply it on small LLMs and assess performance degradation
## References

- **AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration**  
  *by Linjian Ma, Zhengyan Zhang, Yixiao Ge, et al.*  
  arXiv: https://arxiv.org/abs/2306.00978

